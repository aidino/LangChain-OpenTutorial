{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propositions Chunking\n",
    "\n",
    "### Tổng quan\n",
    "\n",
    "Đoạn mã này triển khai phương pháp *proposition chunking*, dựa trên [nghiên cứu của Tony Chen và cộng sự](https://doi.org/10.48550/arXiv.2312.06648). Hệ thống phân tích văn bản đầu vào thành các *propositions* (mệnh đề) mang tính chất nguyên tử, thực tế, độc lập và súc tích, mã hóa các *propositions* này vào một *vectorstore*, có thể được sử dụng sau này để truy xuất.\n",
    "\n",
    "### Các thành phần chính\n",
    "\n",
    "1.  **Document Chunking:** Chia tài liệu thành các phần có thể quản lý được để phân tích.\n",
    "2.  **Proposition Generation:** Sử dụng LLMs để phân tích các *document chunks* thành các *propositions* mang tính thực tế và độc lập.\n",
    "3.  **Proposition Quality Check:** Đánh giá các *propositions* được tạo ra dựa trên độ chính xác, rõ ràng, đầy đủ và súc tích.\n",
    "4.  **Embedding and Vector Store:** *Embedding* cả *propositions* và các *chunks* lớn hơn của tài liệu vào một *vector store* để truy xuất hiệu quả.\n",
    "5.  **Retrieval and Comparison:** Kiểm tra hệ thống truy xuất với các kích thước truy vấn khác nhau và so sánh kết quả từ mô hình dựa trên *proposition* với mô hình dựa trên *chunk* lớn hơn.\n",
    "\n",
    "<img src=\"images/proposition_chunking.svg\" alt=\"Reliable-RAG\" width=\"600\">\n",
    "\n",
    "### Động lực\n",
    "\n",
    "Động lực đằng sau phương pháp *proposition chunking* là xây dựng một hệ thống phân tích tài liệu văn bản thành các *propositions* súc tích, thực tế để truy xuất thông tin chi tiết và chính xác hơn. Việc sử dụng *propositions* cho phép kiểm soát chi tiết hơn và xử lý tốt hơn các truy vấn cụ thể, đặc biệt là để trích xuất kiến thức từ các văn bản chi tiết hoặc phức tạp. Việc so sánh giữa việc sử dụng các *proposition chunks* nhỏ hơn và các *document chunks* lớn hơn nhằm đánh giá hiệu quả của việc truy xuất thông tin chi tiết.\n",
    "\n",
    "### Chi tiết phương pháp\n",
    "\n",
    "1.  **Loading Environment Variables:** Mã bắt đầu bằng cách tải các biến môi trường (ví dụ: khóa API cho dịch vụ LLM) để đảm bảo hệ thống có thể truy cập các tài nguyên cần thiết.\n",
    "\n",
    "2.  **Document Chunking:**\n",
    "    -   Tài liệu đầu vào được chia thành các phần nhỏ hơn (*chunks*) bằng cách sử dụng `RecursiveCharacterTextSplitter`. Điều này đảm bảo rằng mỗi *chunk* có kích thước quản lý được để LLM xử lý.\n",
    "\n",
    "3.  **Proposition Generation:**\n",
    "    -   Các *propositions* được tạo ra từ mỗi *chunk* bằng cách sử dụng LLM (trong trường hợp này, \"llama-3.1-70b-versatile\"). Đầu ra được cấu trúc dưới dạng danh sách các câu lệnh thực tế, độc lập có thể hiểu được mà không cần ngữ cảnh bổ sung.\n",
    "\n",
    "4.  **Quality Check:**\n",
    "    -   Một LLM thứ hai đánh giá chất lượng của các *propositions* bằng cách chấm điểm chúng dựa trên độ chính xác, rõ ràng, đầy đủ và súc tích. Các *propositions* đáp ứng các ngưỡng yêu cầu trong tất cả các danh mục sẽ được giữ lại.\n",
    "\n",
    "5.  **Embedding Propositions:**\n",
    "    -   Các *propositions* vượt qua kiểm tra chất lượng được *embedding* vào một *vector store* bằng mô hình `OllamaEmbeddings`. Điều này cho phép truy xuất các *propositions* dựa trên sự tương đồng khi có truy vấn.\n",
    "\n",
    "6.  **Retrieval and Comparison:**\n",
    "    -   Hai hệ thống truy xuất được xây dựng: một hệ thống sử dụng các *proposition-based chunks* và một hệ thống khác sử dụng các *document chunks* lớn hơn. Cả hai đều được kiểm tra với một số truy vấn để so sánh hiệu suất và độ chính xác của kết quả trả về.\n",
    "\n",
    "### Lợi ích\n",
    "\n",
    "-   **Granularity (Độ chi tiết):** Bằng cách phân tích tài liệu thành các *propositions* thực tế nhỏ, hệ thống cho phép truy xuất rất cụ thể, giúp trích xuất câu trả lời chính xác từ các tài liệu lớn hoặc phức tạp dễ dàng hơn.\n",
    "-   **Quality Assurance (Đảm bảo chất lượng):** Việc sử dụng LLM kiểm tra chất lượng đảm bảo rằng các *propositions* được tạo ra đáp ứng các tiêu chuẩn cụ thể, cải thiện độ tin cậy của thông tin được truy xuất.\n",
    "-   **Flexibility in Retrieval (Tính linh hoạt trong truy xuất):** Việc so sánh giữa truy xuất dựa trên *proposition* và truy xuất dựa trên *chunk* lớn hơn cho phép đánh giá sự đánh đổi giữa độ chi tiết và ngữ cảnh rộng hơn trong kết quả tìm kiếm.\n",
    "\n",
    "### Triển khai\n",
    "\n",
    "1.  **Proposition Generation:** LLM được sử dụng kết hợp với một *prompt* tùy chỉnh để tạo ra các câu lệnh thực tế từ các *document chunks*.\n",
    "2.  **Quality Checking:** Các *propositions* được tạo ra được chuyển qua một hệ thống chấm điểm đánh giá độ chính xác, rõ ràng, đầy đủ và súc tích.\n",
    "3.  **Vector Store Integration:** Các *propositions* được lưu trữ trong một *vector store* FAISS sau khi được *embedding* bằng một mô hình *embedding* được đào tạo trước, cho phép tìm kiếm và truy xuất dựa trên sự tương đồng hiệu quả.\n",
    "4.  **Query Testing:** Nhiều truy vấn kiểm tra được thực hiện đối với các *vector stores* (*proposition-based* và *chunks* lớn hơn) để so sánh hiệu suất truy xuất.\n",
    "\n",
    "### Tóm tắt\n",
    "\n",
    "Đoạn mã này trình bày một phương pháp mạnh mẽ để phân tích một tài liệu thành các *propositions* độc lập bằng cách sử dụng LLMs. Hệ thống thực hiện kiểm tra chất lượng trên mỗi *proposition*, *embedding* chúng trong một *vector store* và truy xuất thông tin phù hợp nhất dựa trên truy vấn của người dùng. Khả năng so sánh các *propositions* chi tiết với các *document chunks* lớn hơn cung cấp thông tin chi tiết về phương pháp nào mang lại kết quả chính xác hoặc hữu ích hơn cho các loại truy vấn khác nhau. Phương pháp này nhấn mạnh tầm quan trọng của việc tạo và truy xuất *propositions* chất lượng cao để trích xuất thông tin chính xác từ các tài liệu phức tạp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "llm_model_name='qwen2.5:7b'\n",
    "llm = ChatOllama(model=llm_model_name, temperature=0)\n",
    "\n",
    "embedding_model='bge-m3:latest'\n",
    "embeddings = OllamaEmbeddings(model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pdf file and split file to chunks\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "PDF_FILE_PATH='law_gtdb/data/law_gtdb_35.pdf'\n",
    "chunk_size = 200\n",
    "chunk_overlap = 50\n",
    "\n",
    "loader = PyPDFLoader(PDF_FILE_PATH)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "  chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "doc_split = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CÔNG BÁO/Số 983 + 984/Ngày 25-8-2024 3 \\n \\n \\nQUỐC HỘI \\n \\nCỘNG HÒA XÃ HỘI CHỦ NGHĨA VIỆT NAM \\nĐộc lập - Tự do - Hạnh phúc \\nLuật số: 35/2024/QH15  \\n \\nLUẬT \\nĐƯỜNG BỘ'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_split[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'PyPDF',\n",
       " 'creator': 'PyPDF',\n",
       " 'creationdate': '2024-09-20T10:02:58+07:00',\n",
       " 'moddate': '2024-09-20T10:02:58+07:00',\n",
       " 'source': 'law_gtdb/data/law_gtdb_35.pdf',\n",
       " 'total_pages': 69,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_split[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'PyPDF',\n",
       " 'creator': 'PyPDF',\n",
       " 'creationdate': '2024-09-20T10:02:58+07:00',\n",
       " 'moddate': '2024-09-20T10:02:58+07:00',\n",
       " 'source': 'law_gtdb/data/law_gtdb_35.pdf',\n",
       " 'total_pages': 69,\n",
       " 'page': 0,\n",
       " 'page_label': '1',\n",
       " 'chunk_id': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, doc in enumerate(doc_split):\n",
    "  doc.metadata['chunk_id'] = i+1\n",
    "\n",
    "doc_split[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Data model\n",
    "class GeneratePropositions(BaseModel):\n",
    "    \"\"\"List of all the propositions in a given document\"\"\"\n",
    "\n",
    "    propositions: List[str] = Field(\n",
    "        description=\"List of propositions (factual, self-contained, and concise information)\"\n",
    "    )\n",
    "\n",
    "structured_llm= llm.with_structured_output(GeneratePropositions)\n",
    "\n",
    "\n",
    "# Few shot prompting --- We can add more examples to make it good\n",
    "proposition_examples = [\n",
    "    {\"document\": \n",
    "        \"In 1969, Neil Armstrong became the first person to walk on the Moon during the Apollo 11 mission.\", \n",
    "     \"propositions\": \n",
    "        \"['Neil Armstrong was an astronaut.', 'Neil Armstrong walked on the Moon in 1969.', 'Neil Armstrong was the first person to walk on the Moon.', 'Neil Armstrong walked on the Moon during the Apollo 11 mission.', 'The Apollo 11 mission occurred in 1969.']\"\n",
    "    },\n",
    "]\n",
    "\n",
    "example_proposition_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{document}\"),\n",
    "        (\"ai\", \"{propositions}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_proposition_prompt,\n",
    "    examples = proposition_examples,\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"Please break down the following text into simple, self-contained propositions. Ensure that each proposition meets the following criteria:\n",
    "\n",
    "    1. Express a Single Fact: Each proposition should state one specific fact or claim.\n",
    "    2. Be Understandable Without Context: The proposition should be self-contained, meaning it can be understood without needing additional context.\n",
    "    3. Use Full Names, Not Pronouns: Avoid pronouns or ambiguous references; use full entity names.\n",
    "    4. Include Relevant Dates/Qualifiers: If applicable, include necessary dates, times, and qualifiers to make the fact precise.\n",
    "    5. Contain One Subject-Predicate Relationship: Focus on a single subject and its corresponding action or attribute, without conjunctions or multiple clauses.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{document}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "proposition_generator = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'producer': 'PyPDF',\n",
    " 'creator': 'PyPDF',\n",
    " 'creationdate': '2024-09-20T10:02:58+07:00',\n",
    " 'moddate': '2024-09-20T10:02:58+07:00',\n",
    " 'source': 'law_gtdb/data/law_gtdb_35.pdf',\n",
    " 'total_pages': 69,\n",
    " 'page': 0,\n",
    " 'page_label': '1',\n",
    " 'chunk_id': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'law_gtdb/data/law_gtdb_35.pdf'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_split[0].metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 33/930 [12:21<4:38:07, 18.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 90/930 [34:31<5:19:46, 22.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 114/930 [44:45<4:10:17, 18.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 135/930 [51:19<3:57:07, 17.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 180/930 [1:06:43<4:51:36, 23.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 220/930 [1:22:46<4:10:42, 21.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 225/930 [1:24:36<3:54:56, 20.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 238/930 [1:28:51<3:40:16, 19.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 308/930 [1:57:28<4:01:28, 23.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 309/930 [1:57:42<3:31:46, 20.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 358/930 [2:15:31<3:12:01, 20.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 368/930 [2:19:17<3:34:39, 22.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 471/930 [2:59:38<3:23:38, 26.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 512/930 [3:17:29<3:47:18, 32.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 515/930 [3:19:14<3:55:03, 33.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 530/930 [3:27:11<3:42:17, 33.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 549/930 [3:36:48<2:59:10, 28.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 559/930 [3:41:50<2:44:07, 26.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 572/930 [3:46:42<2:10:14, 21.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 608/930 [4:00:45<2:36:14, 29.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 616/930 [4:03:38<1:54:23, 21.86s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "propositions = [] # Store all the propositions from the document\n",
    "\n",
    "for i in tqdm(range(len(doc_split))):\n",
    "    response = proposition_generator.invoke({\"document\": doc_split[i].page_content}) # Creating proposition\n",
    "    try:\n",
    "      for proposition in response.propositions:\n",
    "        propositions.append(\n",
    "          Document(\n",
    "            page_content=proposition, \n",
    "            metadata={\"source\": doc_split[i].metadata['source'], \n",
    "                      \"total_pages\": doc_split[i].metadata['total_pages'],\n",
    "                      \"page\": doc_split[i].metadata['page'],\n",
    "                      \"page_label\": doc_split[i].metadata['page_label'],\n",
    "                      \"chunk_id\": i+1}\n",
    "            )\n",
    "          )\n",
    "    except:\n",
    "      print(\"Parser error \")\n",
    "      continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
